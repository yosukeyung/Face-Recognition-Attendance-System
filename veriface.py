# -*- coding: utf-8 -*-
"""Veriface.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jfnYRgcAlk8wVLtjDBr4w8_KzwsKB1Q9

#1. Install library yang dibutuhkan
"""

# !pip install insightface

"""insightface adalah library inti yang kami gunakan untuk melakukan pengenalan wajah. insightface menyediakan model deep learning (ArcFace) untuk menghasilkan vektor numerik unik dari wajah (embedding). cara kerja insightface adalah mengidentifikasi dan mengenali wajah, bukan hanya mendeteksi. insightface juga menyediakan detektor wajah yang cepat (MTCNN) untuk menemukan wajah di dalam kamera."""

# !pip install onnxruntime

"""onnxruntime merupakan runtime yang dikembangkan oleh microsoft yang memungkinkan model deep learning (ArcFace) dijalankan di berbagai hardware. onnxruntime memastikan ArcFace berjalan tanpa ketergantungan pada framework pelatihan asli (Pytorch)."""

# !pip install numpy opencv-python

"""numpy merupakan library python paling dasar dan sering digunakan. dalam kasus ini numpy menyediakan array untuk menyimpan hasil dari vektor face embedding.
opencv merupakan library computer version yang digunakan untuk memproses gambar dan video, termasuk mengakses kamera dan menampilkan output visual
"""

# !pip install Pillow

"""pillow merupakan library standart python yang digunakan untuk membuka, memanipulasi dan menyimpan berbagai format gambar (JPG.JPEG,PNG)"""

# # mesin AI utama
import insightface
from insightface.app import FaceAnalysis

import numpy as np #operasi vektor
import cv2 #akses kamera
import os #operation system. digunakan untuk membangun path
import pickle #penyimpanan model ke .pkl
# from PIL import Image #memanipulasi gambar
from scipy.spatial.distance import cosine #metode klasifikasi untuk menghitung jarak cosine. jarak ini menentukan seberapa mirip wajah yang terdeteksi dengan wajah yang ada di database
import time #digunakan untuk menunda eksekusi sejenak, untuk memberikan waktu bagi hardware untuk bersiap
import csv #untuk membuat log absensi
from datetime import datetime #digunakan untuk mendapatkan waktu saat absen
# from google.colab.patches import cv2_imshow # Untuk menampilkan gambar di Colab
# from google.colab import files # Import library untuk download file
# import matplotlib.pyplot as plt #library untuk grafik
# import random # Diperlukan untuk memilih path gambar secara dinamis

"""#2. mengambil model dari library insightface

Inisialisasi model FaceAnalysis (menggunakan ArcFace dan MTCNN)
"""

# app = FaceAnalysis(allowed_modules=['detection', 'recognition'])
# # Ini akan mendeteksi wajah (MTCNN) dan menghasilkan embedding (ArcFace).

# app.prepare(ctx_id=0, det_size=(640, 640))
# #ctx_id=0 berarti menggunakan gpu
# #det_size berfungsi untuk menentukan ukuran input yang akan digunakan untuk mendeteksi wajah. ukuran yang lebih besar dapat meningkatkan akurasi deteksi tetapi memperlambat pemrosesan
# print("Model ArcFace dan MTCNN berhasil dimuat.")

"""#3. Tahap ekstraksi embedding wajah"""

# # Tentukan lokasi folder data wajah yang berbeda
# PATH_DATA_LIST = [
#     'mahasiswa1',
#     'mahasiswa5',
#     'mahasiswa20'
# ]

# # Tentukan nama file output .pkl yang berbeda
# OUTPUT_FILE_LIST = [
#     'absensi_1.pkl',
#     'absensi_5.pkl',
#     'absensi_20.pkl'
# ]

# # PROSES EKSTRAKSI UNTUK SETIAP PATH

# for PATH_DATA, OUTPUT_FILE in zip(PATH_DATA_LIST, OUTPUT_FILE_LIST):

#     known_face_encodings = [] # Reset list untuk model baru
#     known_face_names = [] # Reset list untuk model baru

#     print(f"\n--- Memulai proses ekstraksi untuk folder: {PATH_DATA} ---")

#     for person_name in os.listdir(PATH_DATA):
#         person_dir = os.path.join(PATH_DATA, person_name)
#         if os.path.isdir(person_dir):
#             print(f"Memproses wajah: {person_name}")
#             for filename in os.listdir(person_dir):
#                 if filename.lower().endswith(('.jpeg', '.png','jpg')): # Tambahkan .lower() agar case insensitive
#                     image_path = os.path.join(person_dir, filename)
#                     try:
#                         img = cv2.imread(image_path) # Muat gambar

#                         # Cek jika gambar gagal dimuat
#                         if img is None:
#                             print(f"Peringatan: Gagal memuat gambar di {image_path}. Lewati.")
#                             continue

#                         faces = app.get(img)
#                         # mendeteksi wajah (MTCNN) dan menghasilkan vektor embedding (ArcFace) untuk wajah tersebut.

#                         if len(faces) == 1:
#                             # Asumsikan 1 wajah per gambar di database
#                             # faces[0].embedding adalah vektor embedding ArcFace
#                             known_face_encodings.append(faces[0].embedding)
#                             known_face_names.append(person_name)
#                         elif len(faces) > 1:
#                             print(f"Peringatan: Lebih dari satu wajah terdeteksi di {image_path}. Lewati.")
#                         else:
#                             print(f"Wajah tidak terdeteksi di {image_path}. Lewati.")
#                     except Exception as e:
#                         print(f"Gagal memproses {image_path}: {e}")

#     print(f"Ekstraksi embedding selesai untuk {PATH_DATA}.")

#     # Simpan encoding dan nama ke model pickle
#     data = {"encodings": np.array(known_face_encodings), "names": known_face_names}

#     # Simpan ke file output spesifik
#     with open(OUTPUT_FILE, "wb") as f:
#         f.write(pickle.dumps(data))

#     print(f"Data encoding absensi telah disimpan di: {OUTPUT_FILE}")

"""#4. Komponen Uji

Menentukan Threshold setiap datasets
"""

# DATABASE_FILES = {
#     '1 Foto/Orang': 'absensi_1.pkl',
#     '5 Foto/Orang': 'absensi_5.pkl',
#     '20 Foto/Orang': 'absensi_20.pkl'
# }
# PATH_IMPOSTER_FOLDER = '/content/uknown' #path ke uknown/imposter folder
# SAFETY_BUFFER = 0.01 #konstanta untuk mengurangi nilai final threshold
# FINAL_THRESHOLDS = {}

# # FUNGSI UNTUK ANALISIS FRR (KNOWN vs KNOWN)
# def calculate_max_genuine_distance(encodings, names):
#     # Menghitung D_max_genuine: Jarak Kosinus Terbesar antar wajah ORANG YANG SAMA.
#     genuine_distances = []
#     n = len(encodings)

#     for i in range(n):
#         for j in range(i + 1, n):
#             if names[i] == names[j]:
#                 distance = cosine(encodings[i], encodings[j])
#                 genuine_distances.append(distance)

#     if genuine_distances:
#         D_max_genuine = np.max(genuine_distances)
#     else:
#         D_max_genuine = None

#     return D_max_genuine, len(genuine_distances)

# # FUNGSI UNTUK ANALISIS FAR (KNOWN vs UNKNOWN)
# def calculate_min_imposter_distance(known_encodings, unknown_encodings):
#     #Menghitung D_min_unknown: Jarak Kosinus Terkecil antar wajah ORANG ASING vs Database.
#     all_imposter_distances = []

#     for unknown_emb in unknown_encodings:
#         for known_emb in known_encodings:
#             distance = cosine(unknown_emb, known_emb)
#             all_imposter_distances.append(distance)

#     if all_imposter_distances:
#         D_min_unknown = np.min(all_imposter_distances)
#     else:
#         D_min_unknown = None

#     return D_min_unknown, len(all_imposter_distances)


# #Fungsi Utama Pengujian Model (MODIFIKASI LOGIKA PENYIMPANAN THRESHOLD)

# def run_threshold_analysis(pkl_file, label, imposter_folder, safety_buffer, unknown_encodings_cache):

#     print(f"\n==================== Menganalisis Model: {label} ====================")

#     # Muat Database Wajah Terdaftar (KNOWN)
#     try:
#         with open(pkl_file, "rb") as f:
#             data = pickle.loads(f.read())
#         known_encodings = data["encodings"]
#         known_face_names = data["names"]
#         print(f"Database {label} berhasil dimuat. Total: {len(known_encodings)} embeddings.")
#     except FileNotFoundError:
#         print(f"ERROR FATAL: Database terdaftar '{pkl_file}' tidak ditemukan. Lewati.")
#         return None
#     except Exception as e:
#         print(f"ERROR memuat database {pkl_file}: {e}. Lewati.")
#         return None

#     # Analisis FRR (D_max_genuine)
#     D_max_genuine, num_genuine = calculate_max_genuine_distance(known_encodings, known_face_names)

#     # Analisis FAR (D_min_unknown)
#     unknown_encodings = unknown_encodings_cache
#     D_min_unknown = None
#     far_threshold = None

#     if len(unknown_encodings) > 0:
#         D_min_unknown, num_imposter_comparisons = calculate_min_imposter_distance(known_encodings, unknown_encodings)
#         far_threshold = D_min_unknown - safety_buffer
#         print(f"Total {num_imposter_comparisons} perbandingan Imposter dilakukan.")
#     else:
#         print("Peringatan: Tidak ada embeddings UNKNOWN yang tersedia untuk analisis FAR.")

#     # HASIL AKHIR & REKOMENDASI

#     final_recommended_threshold = None

#     # Hasil FRR
#     print("\n[A] BATAS TOLERANSI FRR (Gagal Absen)")
#     if D_max_genuine is not None:
#         frr_threshold = D_max_genuine + safety_buffer
#         print(f"Jarak Kosinus Genuine Terbesar (D_max): {D_max_genuine:.4f} ({num_genuine} pasangan)")
#         print(f"Rekomendasi Ambang Batas FRR (Threshold > D_max): {frr_threshold:.4f}")
#     else:
#         print(" -> Gagal menghitung D_max. Database KNOWN harus memiliki minimal 2 foto per orang (1 Foto/Orang).")

#     # Hasil FAR
#     print("\n[B] BATAS KEAMANAN FAR (Kecurangan)")
#     if D_min_unknown is not None:
#         print(f"Jarak Kosinus Imposter Terkecil (D_min): {D_min_unknown:.4f}")
#         print(f"Rekomendasi Ambang Batas FAR (Threshold < D_min): {far_threshold:.4f}")
#     else:
#         print(" -> Gagal menghitung D_min. Embeddings UNKNOWN tidak cukup.")

#     # PENYIMPANAN THRESHOLD AKHIR

#     if D_min_unknown is not None:
#         # Pengecekan 1: Jika D_max (FRR) tersedia dan D_min (FAR) tersedia
#         if D_max_genuine is not None:
#             if D_max_genuine < D_min_unknown:
#                 # Skenario 1: Terpisah (Optimal EER-Like)
#                 eer_like_threshold = (D_max_genuine + D_min_unknown) / 2
#                 final_recommended_threshold = eer_like_threshold
#                 print("\n--- REKOMENDASI AKHIR ---")
#                 print("Status: Jarak FAR dan FRR terpisah.")
#                 print(f"Threshold Optimal (EER-Like, Seimbang): {eer_like_threshold:.4f}")
#             else:
#                 # Skenario 2: Tumpang Tindih (Pilih FAR-Aman)
#                 final_recommended_threshold = far_threshold
#                 print("\n--- REKOMENDASI AKHIR ---")
#                 print(f"Status: Jarak FAR dan FRR tumpang tindih! Pilih Threshold FAR-Aman: {far_threshold:.4f}.")

#         # Pengecekan 2: Jika D_max (FRR) TIDAK tersedia (Kasus 1 Foto/Orang)
#         else:
#             # Skenario 3: Hanya ada FAR (Pilih FAR-Aman)
#             final_recommended_threshold = far_threshold
#             print("\n--- REKOMENDASI AKHIR ---")
#             print("Status: Hanya D_min (FAR) yang tersedia. Mengutamakan Keamanan.")
#             print(f"Threshold Final (FAR-Aman): {far_threshold:.4f}")


#     return final_recommended_threshold

# # EKSTRAKSI EMBEDDINGS UNKNOWN HANYA SEKALI

# unknown_encodings_cache = []
# print(f"\nMemproses foto dari folder UNKNOWN: {PATH_IMPOSTER_FOLDER}...")

# if not os.path.exists(PATH_IMPOSTER_FOLDER):
#     print(f"ERROR: Folder imposter '{PATH_IMPOSTER_FOLDER}' tidak ditemukan.")
# else:
#     for dirpath, dirnames, filenames in os.walk(PATH_IMPOSTER_FOLDER):
#         for filename in filenames:
#             if filename.lower().endswith(('.jpeg', '.png', '.jpg')):
#                 image_path = os.path.join(dirpath, filename)
#                 img = cv2.imread(image_path)
#                 if img is None: continue

#                 faces = app.get(img)
#                 if len(faces) == 1:
#                     unknown_encodings_cache.append(faces[0].embedding)

#     print(f"Total {len(unknown_encodings_cache)} embeddings UNKNOWN berhasil diekstrak.")


# # EKSEKUSI PENGUJIAN UNTUK SEMUA MODEL DAN SIMPAN HASIL

# print("\n\n#################### START KOMPARASI MODEL ####################")

# for label, pkl_file in DATABASE_FILES.items():
#     recommended_threshold = run_threshold_analysis(
#         pkl_file,
#         label,
#         PATH_IMPOSTER_FOLDER,
#         SAFETY_BUFFER,
#         unknown_encodings_cache
#     )

#     if recommended_threshold is not None:
#         FINAL_THRESHOLDS[label] = recommended_threshold
#         print(f"✅ THRESHOLD AKHIR ({label}): {recommended_threshold:.4f} disimpan.")
#     else:
#         FINAL_THRESHOLDS[label] = None
#         print(f"❌ GAGAL: Threshold untuk {label} tidak dapat ditentukan.")

# print("\n#################### HASIL AKHIR ####################")
# for label, threshold in FINAL_THRESHOLDS.items():
#     print(f"[{label}]: Threshold = {threshold:.4f}" if threshold is not None else f"[{label}]: Threshold GAGAL")

"""Similarity Score"""

# # Mengambil data Threshold
# FINAL_THRESHOLDS = {
#     '1 Foto': FINAL_THRESHOLDS['1 Foto/Orang'],
#     '5 Foto': FINAL_THRESHOLDS['5 Foto/Orang'],
#     '20 Foto': FINAL_THRESHOLDS['20 Foto/Orang']
# }
# # Threshold dasar
# STATIC_THRESHOLD = 0.55

# # Lokasi File Database yang Sudah Diekstrak (.pkl)
# DATABASE_FILES = {
#     '1 Foto': 'absensi_1.pkl',
#     '5 Foto': 'absensi_5.pkl',
#     '20 Foto': 'absensi_20.pkl'
# }

# # Lokasi Folder Gambar Uji (Unseen Test Set)
# PATH_TEST_DATA = 'unseen'

# # FUNGSI PENGUJIAN AKURASI (MODIFIKASI: Menerima Threshold)

# def calculate_average_similarity(pkl_file_path, test_data_path, threshold_to_use):
#     # Memuat satu database dan mengujinya terhadap seluruh test set.

#     try:
#         with open(pkl_file_path, "rb") as f:
#             data = pickle.loads(f.read())
#         known_encodings = data["encodings"]
#         known_names = data["names"]
#     except FileNotFoundError:
#         print(f"ERROR: Database {pkl_file_path} tidak ditemukan.")
#         return None

#     # Set untuk menyimpan skor (Nama: [Skor1, Skor2, ...])
#     similarity_scores = {}
#     correct_acceptances = 0
#     total_tests = 0

#     for person_name in os.listdir(test_data_path):
#         person_test_dir = os.path.join(test_data_path, person_name)
#         if not os.path.isdir(person_test_dir):
#             continue

#         person_scores = []

#         for filename in os.listdir(person_test_dir):
#             if filename.lower().endswith(('.jpeg', '.png', '.jpg')):
#                 image_path = os.path.join(person_test_dir, filename)
#                 img = cv2.imread(image_path)

#                 # Cek jika gambar valid dan wajah terdeteksi
#                 if img is None: continue

#                 faces = app.get(img)
#                 if len(faces) != 1: continue

#                 total_tests += 1
#                 current_embedding = faces[0].embedding
#                 min_distance = 1.0
#                 best_match_name = "Tidak Dikenal"

#                 # Cari kecocokan terbaik di database yang sedang diuji
#                 for i, known_encoding in enumerate(known_encodings):
#                     distance = cosine(current_embedding, known_encoding)
#                     if distance < min_distance:
#                         min_distance = distance
#                         if distance < threshold_to_use: # Gunakan threshold spesifik
#                             best_match_name = known_names[i]

#                 # Hitung Similarity Score
#                 similarity_score = 1 - min_distance
#                 person_scores.append(similarity_score)

#                 # Cek Akurasi (True Acceptance): Wajah teruji ada di DB dan diterima
#                 if person_name == best_match_name:
#                      correct_acceptances += 1


#         if person_scores:
#             similarity_scores[person_name] = np.mean(person_scores)

#     # Hitung rata-rata keseluruhan dari semua skor individu
#     all_scores = [score for scores_list in similarity_scores.values() for score in (scores_list,)]

#     if not all_scores:
#         print(f"Peringatan: Tidak ada wajah yang berhasil diuji di {pkl_file_path}.")
#         return None, None

#     # Menghitung Akurasi berdasarkan True Acceptance
#     accuracy_at_threshold = (correct_acceptances / total_tests) * 100 if total_tests > 0 else 0

#     return np.mean(all_scores), accuracy_at_threshold

# # EKSEKUSI PENGUJIAN

# results = {}
# accuracy_results = {}
# thresholds_used = {} # Menyimpan threshold yang benar-benar digunakan

# print("Memulai Uji Komparatif Dataset dengan Threshold Dinamis...")

# for label, pkl_file in DATABASE_FILES.items():

#     # Ambil threshold spesifik untuk model ini
#     threshold_to_use = FINAL_THRESHOLDS.get(label, STATIC_THRESHOLD)
#     thresholds_used[label] = threshold_to_use

#     # Jalankan pengujian dengan threshold dinamis
#     avg_score, acc_at_threshold = calculate_average_similarity(pkl_file, PATH_TEST_DATA, threshold_to_use)

#     if avg_score is not None:
#         results[label] = avg_score
#         accuracy_results[label] = acc_at_threshold
#         print(f"-> {label}: Threshold = {threshold_to_use:.4f}, Rata-rata Similarity Score = {avg_score:.4f}, Akurasi = {acc_at_threshold:.2f}%")

# # PEMBUATAN GRAFIK PERBANDINGAN

# if not results:
#     print("ERROR: Tidak ada hasil yang cukup untuk membuat grafik.")
#     exit()

# names = list(results.keys())
# values = list(results.values())

# plt.figure(figsize=(9, 6))

# # Membuat Bar Chart
# bars = plt.bar(names, values, color=['lightcoral', 'skyblue', 'darkgreen'], width=0.6)

# # Menambahkan Label dan Judul
# plt.ylabel('Rata-rata Similarity Score (1.0 = Sempurna)', fontsize=12)
# plt.title('Pengujian Komparatif Keandalan Model dengan Threshold Dinamis', fontsize=14, pad=15)
# plt.ylim(0.0, 1.0)

# # Menambahkan Garis Threshold Dinamis
# for name in names:
#     cos_t = thresholds_used[name]
#     sim_t = 1 - cos_t

#     # Mendapatkan posisi bar untuk menempatkan label di bawahnya
#     bar_index = names.index(name)
#     bar_pos = bars[bar_index].get_x() + bars[bar_index].get_width() / 2.0

#     # Menambahkan Garis dan Label Threshold
#     plt.axhline(sim_t, color=bars[bar_index].get_facecolor(), linestyle=':', linewidth=1, alpha=0.7)

#     # Menambahkan label Similarity Threshold (1 - Cosine Threshold) di bawah bar
#     plt.text(bar_pos, sim_t - 0.04,
#              f'T: {sim_t:.3f}\n(Cos: {cos_t:.3f})',
#              ha='center', va='top', fontsize=9,
#              color=bars[bar_index].get_facecolor(),
#              fontweight='bold')

# # Membuat legend hanya untuk bar
# handles, labels_legend = plt.gca().get_legend_handles_labels()
# # Hapus label axhline dari legend
# plt.legend(handles[:len(names)], names, loc='upper left')


# # Menambahkan Nilai Score di atas Bar
# for bar in bars:
#     yval = bar.get_height()
#     plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.02, f'{yval:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')

# FILE_NAME = 'perbandingan_akurasi_dataset_dinamis_final.png'
# plt.savefig(FILE_NAME)
# print(f"\nGrafik perbandingan berhasil disimpan sebagai {FILE_NAME}")
# print("\nCatatan: Threshold yang digunakan untuk setiap model diplot sebagai garis putus-putus berwarna.")

"""Latency"""

# # Lokasi File Database yang Sudah Diekstrak (.pkl)
# DATABASE_FILES = {
#     '1 Foto/Orang': 'absensi_1.pkl',
#     '5 Foto/Orang': 'absensi_5.pkl',
#     '20 Foto/Orang': 'absensi_20.pkl'
# }

# # Lokasi Folder Uji (untuk mengambil satu gambar input)
# PATH_TEST_DATA = 'unseen'
# NUM_TRIALS = 50 # Jumlah simulasi yang dijalankan untuk mendapatkan rata-rata waktu yang stabil

# def find_first_image_path(root_dir):
#     # Mencari path gambar pertama yang valid (.jpg, .jpeg, .png) di dalam struktur folder.

#     # os.walk akan menelusuri folder secara rekursif
#     for dirpath, dirnames, filenames in os.walk(root_dir):
#         # Filter file gambar
#         image_files = [f for f in filenames if f.lower().endswith(('.jpeg', '.png', '.jpg'))]

#         if image_files:
#             # Mengambil path lengkap dari file gambar pertama yang ditemukan
#             return os.path.join(dirpath, image_files[0])

#     raise FileNotFoundError(f"Tidak ditemukan file gambar di dalam folder: {root_dir}")

# # Mendapatkan Path Gambar Uji Secara Dinamis
# try:
#     TEST_IMAGE_PATH = find_first_image_path(PATH_TEST_DATA)
#     print(f"Menggunakan gambar uji: {TEST_IMAGE_PATH}")
# except FileNotFoundError as e:
#     print(f"ERROR FATAL: {e}")
#     exit()

# try:

#     # Muat gambar Uji
#     test_img = cv2.imread(TEST_IMAGE_PATH)

#     # Cek apakah wajah terdeteksi pada gambar Uji
#     test_faces = app.get(test_img)
#     if len(test_faces) != 1:
#         raise ValueError("Gambar Uji harus memiliki tepat 1 wajah terdeteksi.")

#     INTRUDER_EMBEDDING = test_faces[0].embedding
#     print("Inisialisasi berhasil. Embedding uji siap diukur waktunya.")

# except Exception as e:
#     print(f"ERROR FATAL SAAT INISIALISASI: {e}")
#     exit()

# # FUNGSI PENGUKURAN WAKTU END-TO-END

# def measure_unseen_recognition_time(pkl_file_path, intruder_embedding, num_trials):

#     try:
#         with open(pkl_file_path, "rb") as f:
#             data = pickle.loads(f.read())
#         known_encodings = data["encodings"]
#     except FileNotFoundError:
#         return None

#     total_time = 0
#     total_embeddings = len(known_encodings)

#     for _ in range(num_trials):
#         min_distance = 1.0
#         start_time = time.perf_counter()

#         # Proses Perbandingan Jarak Cosine
#         for known_encoding in known_encodings:
#             distance = cosine(intruder_embedding, known_encoding)
#             if distance < min_distance:
#                 min_distance = distance

#         end_time = time.perf_counter()
#         total_time += (end_time - start_time)

#     avg_time_ms = (total_time / num_trials) * 1000

#     return avg_time_ms, total_embeddings

# # EKSEKUSI UJI KECEPATAN (Sama seperti sebelumnya)

# comparison_results = {}
# embedding_counts = {}

# print(f"\n--- Memulai Uji Kecepatan 'Unseen Face' dengan {NUM_TRIALS} percobaan ---")

# for label, pkl_file in DATABASE_FILES.items():
#     result = measure_unseen_recognition_time(pkl_file, INTRUDER_EMBEDDING, NUM_TRIALS)
#     if result is not None:
#         avg_time_ms, total_count = result
#         comparison_results[label] = avg_time_ms
#         embedding_counts[label] = total_count
#         print(f"-> {label}: Waktu rata-rata perbandingan: {avg_time_ms:.4f} ms (Total {total_count} embeddings)")


# # PEMBUATAN GRAFIK KECEPATAN

# if not comparison_results:
#     print("ERROR: Tidak ada hasil kecepatan yang cukup untuk membuat grafik.")
#     exit()

# names = list(comparison_results.keys())
# times = list(comparison_results.values())
# counts = [embedding_counts[name] for name in names]

# plt.figure(figsize=(10, 6))

# # Membuat Bar Chart
# bars = plt.bar(names, times, color=['lightcoral', 'skyblue', 'darkgreen'], width=0.6)

# # Menambahkan Label dan Judul
# plt.ylabel('Waktu Pengenalan Rata-Rata (milidetik)', fontsize=12)
# plt.title('Waktu Transaksi Penuh untuk Memastikan Wajah Tidak Dikenal', fontsize=14, pad=15)
# plt.xlabel('Skenario Database (Total Embeddings: ' + f'{counts[0]}, {counts[1]}, {counts[2]}' + ')', fontsize=12)

# # Menambahkan Nilai Waktu di atas Bar
# for bar in bars:
#     yval = bar.get_height()
#     plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.001, f'{yval:.3f} ms', ha='center', va='bottom', fontsize=11, fontweight='bold')

# FILE_NAME_SPEED = 'perbandingan_kecepatan_unseen_final.png'
# plt.savefig(FILE_NAME_SPEED)
# print(f"\nGrafik perbandingan kecepatan 'Unseen Face' berhasil disimpan sebagai {FILE_NAME_SPEED}")

"""#5. Testing

Testing dengan cara upload foto
"""

# # 1. Muat data encoding yang sudah disimpan
# print("Memuat data encoding...")
# with open("absensi_5.pkl", "rb") as f:
#     data = pickle.loads(f.read())
# known_face_encodings = data["encodings"]
# known_face_names = data["names"]

# # 2. Muat gambar yang akan diuji (Absen)
# TEST_IMAGE_PATH = '[path]'
# test_img = cv2.imread(TEST_IMAGE_PATH)

# # 3. Deteksi dan Cari Wajah di Gambar Uji
# test_faces = app.get(test_img)

# # Jarak Cosine 0 berarti wajah identik, 1 berarti sangat berbeda.
# COSINE_THRESHOLD = 0.6552
# absen_status = {}

# for face in test_faces:
#     # Ambil embedding wajah yang terdeteksi
#     current_embedding = face.embedding

#     name = "Tidak Dikenal"
#     min_distance = 1.0 # Jarak Cosine maksimal

#     # Bandingkan embedding wajah saat ini dengan semua embedding di database
#     for i, known_encoding in enumerate(known_face_encodings):
#         # Hitung Jarak Cosine
#         distance = cosine(current_embedding, known_encoding)

#         if distance < min_distance:
#             min_distance = distance

#             if min_distance < COSINE_THRESHOLD:
#                 name = known_face_names[i]

#         min_distance

#     # Catat status absensi
#     if name != "Tidak Dikenal":
#         color = (0, 255, 0) # Hijau
#     else:
#         color = (0, 0, 255) # Merah

#     # Gambar kotak dan label (Gunakan koordinat dari objek face)
#     bbox = face.bbox.astype(np.int32)

#     cv2.rectangle(test_img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, 2)
#     cv2.rectangle(test_img, (bbox[0], bbox[3] - 30), (bbox[2], bbox[3]), color, cv2.FILLED)
#     font = cv2.FONT_HERSHEY_DUPLEX
#     text_label = f"{name} ({min_distance:.2f})"
#     cv2.putText(test_img, text_label, (bbox[0] + 6, bbox[3] - 6), font, 0.7, (0, 0, 0), 1)

# # 4. Tampilkan Hasil Absensi
# cv2_imshow(test_img)

# OUTPUT_IMAGE_NAME = 'hasil_absensi_terdeteksi.jpg'

# try:
#     # Menyimpan gambar yang telah diproses ke file
#     cv2.imwrite(OUTPUT_IMAGE_NAME, test_img)
#     print(f"\nGambar hasil disimpan di: {OUTPUT_IMAGE_NAME}")

#     # Mendownload file ke komputer lokal Anda
#     files.download(OUTPUT_IMAGE_NAME)
#     print("Download gambar hasil telah dimulai.")

# except Exception as e:
#     print(f"\nERROR saat menyimpan atau mendownload file: {e}")

"""Testing secara real-time"""

try:
    with open("absensi_5.pkl", "rb") as f: #membaca model .pkl 5 foto/indvidiu
        data = pickle.loads(f.read())
    known_face_encodings = data["encodings"]
    known_face_names = data["names"]
    print(f"Data encoding absensi berhasil dimuat. Total: {len(known_face_names)} wajah di database.")
except FileNotFoundError:
    print("ERROR: File 'absensi_arcface_encodings.pkl' tidak ditemukan. Program dihentikan.")
    exit()

# Inisialisasi model InsightFace
app = FaceAnalysis(allowed_modules=['detection', 'recognition'])
app.prepare(ctx_id=0, det_size=(640, 640))

#set parameter
CSV_FILE_NAME = 'log_absensi.csv'
COSINE_THRESHOLD =  0.6552 # treshold dari pengujian 5 foto/individu
FRAME_SKIP = 1 # mendeteksi setiap 1 frame
frame_count = 0
absensi_tercatat_hari_ini = set() # Set untuk mencegah absensi ganda dalam 1 sesi

#mencatat data absen ke csv
def log_absensi_csv(nama, status):
    now = datetime.now()
    timestamp = now.strftime("%Y-%m-%d %H:%M:%S")

    # Pencegahan Absensi Ganda
    if nama in absensi_tercatat_hari_ini:
        return

    fieldnames = ['Timestamp', 'Nama', 'Status']
    file_exists = os.path.isfile(CSV_FILE_NAME)

    with open(CSV_FILE_NAME, 'a', newline='') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

        if not file_exists:
            writer.writeheader()

        writer.writerow({
            'Timestamp': timestamp,
            'Nama': nama,
            'Status': status
        })

    absensi_tercatat_hari_ini.add(nama)
    print(f"[{timestamp}] ABSEN BERHASIL: {nama} dicatat ke CSV.")

#membuka kamera
cap = None
for index in [0, 1, 0 + cv2.CAP_DSHOW]:
    temp_cap = cv2.VideoCapture(index)
    time.sleep(1)
    if temp_cap.isOpened():
        cap = temp_cap
        print(f"Kamera berhasil dibuka pada indeks {index}.")
        break
    temp_cap.release()

if cap is None:
    print("ERROR FATAL: Tidak dapat membuka kamera. Pastikan kamera terpasang dan driver sudah benar.")
    exit()

# loop kamera live
print("\n--- Sistem Absensi Aktif (Tekan 'q' untuk keluar) ---")

while True:
    ret, frame = cap.read()
    if not ret:
        break

    frame_count += 1
    if frame_count % FRAME_SKIP == 0:

        faces = app.get(frame)

        for face in faces:
            current_embedding = face.embedding
            name = "Tidak Dikenal"
            min_distance = 1.0

            # Bandingkan dengan Database (Pengenalan)
            for i, known_encoding in enumerate(known_face_encodings):
                distance = cosine(current_embedding, known_encoding)

                if distance < min_distance:
                    min_distance = distance

                    if min_distance < COSINE_THRESHOLD:
                        name = known_face_names[i]

            #print label
            if name != "Tidak Dikenal":
                color = (0, 255, 0) # Hijau
                log_absensi_csv(name, "HADIR") # Catat absensi
            else:
                color = (0, 0, 255) # Merah

            # Gambar Kotak dan Label
            bbox = face.bbox.astype(np.int32)
            cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, 2)
            cv2.rectangle(frame, (bbox[0], bbox[3] - 30), (bbox[2], bbox[3]), color, cv2.FILLED)

            font = cv2.FONT_HERSHEY_DUPLEX
            text_label = f"{name} ({ min_distance:.2f})"
            cv2.putText(frame, text_label, (bbox[0] + 6, bbox[3] - 6), font, 0.7, (255, 255, 255), 1)

    # Tampilkan Hasil
    cv2.imshow('Sistem Absensi Real-Time', frame)

    # Tombol Keluar (Tekan 'q')
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# 6. Bersihkan
cap.release()
cv2.destroyAllWindows()
print("\nSistem Absensi Dihentikan.")
print(f"Log absensi disimpan di file: {CSV_FILE_NAME}")